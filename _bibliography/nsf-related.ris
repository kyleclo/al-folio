TY  - CPAPER
AU  - Krishna, Kalpesh
AU  - Bransom, Erin
AU  - Kuehl, Bailey
AU  - Iyyer, Mohit
AU  - Dasigi, Pradeep
AU  - Cohan, Arman
AU  - Lo, Kyle
TI  - LongEval: Guidelines for Human Evaluation of Faithfulness in Long-form
      Summarization
T2  - EACL
SP  - 1650-1669
PY  - 2023
DA  - 2023/5
PB  - Association for Computational Linguistics
AB  - While human evaluation remains best practice for accurately judging the
      faithfulness of automatically-generated summaries, few solutions exist to
      address the increased difficulty and workload when evaluating long-form
      summaries. Through a survey of 162 papers on long-form summarization, we
      first shed light on current human evaluation practices surrounding
      long-form summaries. We find that 73% of these papers do not perform any
      human evaluation on model-generated summaries, while other works face new
      difficulties that manifest when dealing with long documents (e.g., low
      inter-annotator agreement). Motivated by our survey, we present LongEval,
      a set of guidelines for human evaluation of faithfulness in long-form
      summaries that addresses the following challenges: (1) How can we achieve
      high inter-annotator agreement on faithfulness scores? (2) How can we
      minimize annotator workload while maintaining accurate faithfulness
      scores? and (3) Do humans benefit from automated alignment between summary
      and source snippets? We deploy LongEval in annotation studies on two
      long-form summarization datasets in different domains (SQuALITY and
      PubMed), and we find that switching to a finer granularity of judgment
      (e.g., clause-level) reduces inter-annotator variance in faithfulness
      scores (e.g., std-dev from 18.5 to 6.8). We also show that scores from a
      partial annotation of fine-grained units highly correlates with scores
      from a full annotation workload (0.89 Kendall's tau using 50% judgements).
      We release our human judgments, annotation templates, and software as a
      Python library for future research.
C1  - Dubrovnik, Croatia
UR  - https://aclanthology.org/2023.eacl-main.121
UR  - http://arxiv.org/abs/2301.13298
ER  - 

TY  - JOUR
AU  - Lo, Kyle
AU  - Chang, Joseph Chee
AU  - Head, Andrew
AU  - Bragg, Jonathan
AU  - Zhang, Amy X
AU  - Trier, Cassidy
AU  - Anastasiades, Chloe
AU  - August, Tal
AU  - Authur, Russell
AU  - Bragg, Danielle
AU  - Bransom, Erin
AU  - Cachola, Isabel
AU  - Candra, Stefan
AU  - Chandrasekhar, Yoganand
AU  - Chen, Yen-Sung
AU  - Cheng, Evie (yu-Yen)
AU  - Chou, Yvonne
AU  - Downey, Doug
AU  - Evans, Rob
AU  - Fok, Raymond
AU  - Hu, F Q
AU  - Huff, Regan
AU  - Kang, Dongyeop
AU  - Kim, Tae Soo
AU  - Kinney, Rodney Michael
AU  - Kittur, Aniket
AU  - Kang, Hyeonsu B
AU  - Klevak, Egor
AU  - Kuehl, Bailey
AU  - Langan, Michael
AU  - Latzke, Matt
AU  - Lochner, Jaron
AU  - MacMillan, Kelsey
AU  - Marsh, Eric
AU  - Murray, Tyler
AU  - Naik, Aakanksha
AU  - Nguyen, Ngoc-Uyen
AU  - Palani, Srishti
AU  - Park, Soya
AU  - Paulic, Caroline
AU  - Rachatasumrit, Napol
AU  - Rao, Smita
AU  - Sayre, Paul L
AU  - Shen, Zejiang
AU  - Siangliulue, Pao
AU  - Soldaini, Luca
AU  - Tran, Huy
AU  - van Zuylen, Madeleine
AU  - Wang, Lucy Lu
AU  - Wilhelm, Christopher
AU  - Wu, Caroline M
AU  - Yang, Jiangjiang
AU  - Zamarron, Angele
AU  - Hearst, Marti A
AU  - Weld, Daniel S
TI  - The Semantic Reader Project: Augmenting Scholarly Documents through
      AI-Powered Interactive Reading Interfaces
T2  - ArXiv
VL  - abs/2303.14334
PY  - 2023
DA  - 2023/3/23
AB  - Scholarly publications are key to the transfer of knowledge from scholars
      to others. However, research papers are information-dense, and as the
      volume of the scientific literature grows, the need for new technology to
      support the reading process grows. In contrast to the process of finding
      papers, which has been transformed by Internet technology, the experience
      of reading research papers has changed little in decades. The PDF format
      for sharing research papers is widely used due to its portability, but it
      has significant downsides including: static content, poor accessibility
      for low-vision readers, and difficulty reading on mobile devices. This
      paper explores the question "Can recent advances in AI and HCI power
      intelligent, interactive, and accessible reading interfaces -- even for
      legacy PDFs?" We describe the Semantic Reader Project, a collaborative
      effort across multiple institutions to explore automatic creation of
      dynamic reading interfaces for research papers. Through this project,
      we've developed ten research prototype interfaces and conducted usability
      studies with more than 300 participants and real-world users showing
      improved reading experiences for scholars. We've also released a
      production reading interface for research papers that will incorporate the
      best features as they mature. We structure this paper around challenges
      scholars and the public face when reading research papers -- Discovery,
      Efficiency, Comprehension, Synthesis, and Accessibility -- and present an
      overview of our progress and remaining open challenges.
UR  - http://arxiv.org/abs/2303.14334
ER  - 

TY  - CPAPER
AU  - Wadden, David
AU  - Lin, Shanchuan
AU  - Lo, Kyle
AU  - Wang, Lucy Lu
AU  - van Zuylen, Madeleine
AU  - Cohan, Arman
AU  - Hajishirzi, Hannaneh
TI  - Fact or Fiction: Verifying Scientific Claims
T2  - EMNLP
SP  - 7534-7550
PY  - 2020
DA  - 2020/11
PB  - Association for Computational Linguistics
AB  - We introduce scientific claim verification, a new task to select abstracts
      from the research literature containing evidence that SUPPORTS or REFUTES
      a given scientific claim, and to identify rationales justifying each
      decision. To study this task, we construct SciFact, a dataset of 1.4K
      expert-written scientific claims paired with evidence-containing abstracts
      annotated with labels and rationales. We develop baseline models for
      SciFact, and demonstrate that simple domain adaptation techniques
      substantially improve performance compared to models trained on Wikipedia
      or political news. We show that our system is able to verify claims
      related to COVID-19 by identifying evidence from the CORD-19 corpus. Our
      experiments indicate that SciFact will provide a challenging testbed for
      the development of new systems designed to retrieve and reason over
      corpora containing specialized domain knowledge. Data and code for this
      new task are publicly available at https://github.com/allenai/scifact. A
      leaderboard and COVID-19 fact-checking demo are available at
      https://scifact.apps.allenai.org.
DO  - 10.18653/v1/2020.emnlp-main.609
C1  - Online
UR  - https://aclanthology.org/2020.emnlp-main.609
UR  - http://dx.doi.org/10.18653/v1/2020.emnlp-main.609
ER  - 

TY  - CPAPER
AU  - Cachola, Isabel
AU  - Lo, Kyle
AU  - Cohan, Arman
AU  - Weld, Daniel
TI  - TLDR: Extreme Summarization of Scientific Documents
T2  - Findings of EMNLP
SP  - 4766-4777
PY  - 2020
DA  - 2020/11
PB  - Association for Computational Linguistics
AB  - We introduce TLDR generation, a new form of extreme summarization, for
      scientific papers. TLDR generation involves high source compression and
      requires expert background knowledge and understanding of complex
      domain-specific language. To facilitate study on this task, we introduce
      SCITLDR, a new multi-target dataset of 5.4K TLDRs over 3.2K papers.
      SCITLDR contains both author-written and expert-derived TLDRs, where the
      latter are collected using a novel annotation protocol that produces
      high-quality summaries while minimizing annotation burden. We propose
      CATTS, a simple yet effective learning strategy for generating TLDRs that
      exploits titles as an auxiliary training signal. CATTS improves upon
      strong baselines under both automated metrics and human evaluations. Data
      and code are publicly available at https://github.com/allenai/scitldr.
DO  - 10.18653/v1/2020.findings-emnlp.428
C1  - Online
UR  - https://aclanthology.org/2020.findings-emnlp.428
UR  - http://dx.doi.org/10.18653/v1/2020.findings-emnlp.428
ER  - 

TY  - CPAPER
AU  - Bragg, Jonathan
AU  - Cohan, Arman
AU  - Lo, Kyle
AU  - Beltagy, Iz
A2  - Ranzato, M
A2  - Beygelzimer, A
A2  - Dauphin, Y
A2  - Liang, P S
A2  - Vaughan, J Wortman
TI  - FLEX: Unifying Evaluation for Few-Shot NLP
T2  - NeurIPS
VL  - 34
SP  - 15787-15800
PY  - 2021
DA  - 2021/12
PB  - Curran Associates, Inc.
AB  - Few-shot NLP research is highly active, yet conducted in disjoint research
      threads with evaluation suites that lack challenging-yet-realistic testing
      setups and fail to employ careful experimental design. Consequently, the
      community does not know which techniques perform best or even if they
      outperform simple baselines. In response, we formulate the FLEX
      Principles, a set of requirements and best practices for unified,
      rigorous, valid, and cost-sensitive few-shot NLP evaluation. These
      principles include Sample Size Design, a novel approach to benchmark
      design that optimizes statistical accuracy and precision while keeping
      evaluation costs manageable. Following the principles, we release the FLEX
      benchmark, which includes four few-shot transfer settings, zero-shot
      evaluation, and a public leaderboard that covers diverse NLP tasks. In
      addition, we present UniFew, a prompt-based model for few-shot learning
      that unifies pretraining and finetuning prompt formats, eschewing complex
      machinery of recent prompt-based approaches in adapting downstream task
      formats to language model pretraining objectives. We demonstrate that
      despite simplicity, UniFew achieves results competitive with both popular
      meta-learning and prompt-based approaches.
UR  - https://proceedings.neurips.cc/paper/2021/file/8493eeaccb772c0878f99d60a0bd2bb3-Paper.pdf
ER  - 
