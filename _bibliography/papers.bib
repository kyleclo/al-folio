---
---

@article{Chen2023AreLL,
  title={Are Layout-Infused Language Models Robust to Layout Distribution Shifts? A Case Study with Scientific Documents},
  author={Catherine Chen and Zejiang Shen and Dan Klein and Gabriel Stanovsky and Doug Downey and Kyle Lo},
  booktitle={Findings of ACL},
  year={2023},
  month={jun},
  url={https://arxiv.org/abs/2306.01058},
  abstract={Recent work has shown that infusing layout features into language models (LMs) improves processing of visually-rich documents such as scientific papers. Layout-infused LMs are often evaluated on documents with familiar layout features (e.g., papers from the same publisher), but in practice models encounter documents with unfamiliar distributions of layout features, such as new combinations of text sizes and styles, or new spatial configurations of textual elements. In this work we test whether layout-infused LMs are robust to layout distribution shifts. As a case study we use the task of scientific document structure recovery, segmenting a scientific paper into its structural categories (e.g., "title", "caption", "reference"). To emulate distribution shifts that occur in practice we re-partition the GROTOAP2 dataset. We find that under layout distribution shifts model performance degrades by up to 20 F1. Simple training strategies, such as increasing training diversity, can reduce this degradation by over 35% relative F1; however, models fail to reach in-distribution performance in any tested out-of-distribution conditions. This work highlights the need to consider layout distribution shifts during model evaluation, and presents a methodology for conducting such evaluations.},
  bibtex_show={true},
  arxiv={2306.01058},
  pdf={are-layout-infused-language-models-robust-to-layout-distribution-shifts-a-case-study-with-scientific-documents.pdf},
  preview={are-layout-infused-language-models-robust-to-layout-distribution-shifts-a-case-study-with-scientific-documents.png}
}

@article{Lin2023DecomposingCQ,
  title={Decomposing Complex Queries for Tip-of-the-tongue Retrieval},
  author={Kevin Lin and Kyle Lo and Joseph E. Gonzalez and Dan Klein},
  journal={ArXiv},
  year={2023},
  month={may},
  volume={abs/2305.15053},
  abstract={When re-finding items, users who forget or are uncertain about identifying details often rely on creative strategies for expressing their information needs -- complex queries that describe content elements (e.g., book characters or events), information beyond the document text (e.g., descriptions of book covers), or personal context (e.g., when they read a book). This retrieval setting, called tip of the tongue (TOT), is especially challenging for models heavily reliant on lexical and semantic overlap between query and document text. In this work, we introduce a simple yet effective framework for handling such complex queries by decomposing the query into individual clues, routing those as sub-queries to specialized retrievers, and ensembling the results. This approach allows us to take advantage of off-the-shelf retrievers (e.g., CLIP for retrieving images of book covers) or incorporate retriever-specific logic (e.g., date constraints). We show that our framework incorportating query decompositions into retrievers can improve gold book recall up to 7% relative again for Recall@5 on a new collection of 14,441 real-world query-book pairs from an online community for resolving TOT inquiries.},
  bibtex_show={true},
  arxiv={2305.15053},
  pdf={decomposing-complex-queries-for-tip-of-the-tongue-retrieval.pdf},
  preview={decomposing-complex-queries-for-tip-of-the-tongue-retrieval.png}
}

@article{Newman2023ACQ,
  title={A Controllable QA-based Framework for Decontextualization},
  author={Benjamin Newman and Luca Soldaini and Raymond Fok and Arman Cohan and Kyle Lo},
  journal={ArXiv},
  year={2023},
  month={may},
  volume={abs/2305.14772},
  abstract={Many real-world applications require surfacing extracted snippets to users, whether motivated by assistive tools for literature surveys or document cross-referencing, or needs to mitigate and recover from model generated inaccuracies., Yet, these passages can be difficult to consume when divorced from their original document context. In this work, we explore the limits of LLMs to perform decontextualization of document snippets in user-facing scenarios, focusing on two real-world settings - question answering and citation context previews for scientific documents. We propose a question-answering framework for decontextualization that allows for better handling of user information needs and preferences when determining the scope of rewriting. We present results showing state-of-the-art LLMs under our framework remain competitive with end-to-end approaches. We also explore incorporating user preferences into the system, finding our framework allows for controllability.},
  bibtex_show={true},
  arxiv={2305.14772},
  pdf={a-controllable-qa-based-framework-for-decontextualization.pdf},
  preview={a-controllable-qa-based-framework-for-decontextualization.png}
}

@article{MartinBoyle2023ComplexMS,
  title={Complex Mathematical Symbol Definition Structures: A Dataset and Model for Coordination Resolution in Definition Extraction},
  author={Anna Martin-Boyle and Andrew Head and Kyle Lo and Risham Sidhu and Marti A. Hearst and Dongyeop Kang},
  journal={ArXiv},
  year={2023},
  month={may},
  volume={abs/2305.14660},
  bibtex_show={true},
  arxiv={2305.14660},
  pdf={complex-mathematical-symbol-definition-structures-a-dataset-and-model-for-coordination-resolution-in-definition-extraction.pdf},
  preview={complex-mathematical-symbol-definition-structures-a-dataset-and-model-for-coordination-resolution-in-definition-extraction.png}
}

@inproceedings{krishna-etal-2023-longeval,
    title = "{L}ong{E}val: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization",
    author = "Krishna, Kalpesh  and
      Bransom, Erin  and
      Kuehl, Bailey  and
      Iyyer, Mohit  and
      Dasigi, Pradeep  and
      Cohan, Arman  and
      Lo, Kyle",
    booktitle = "EACL",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.121",
    pages = "1650--1669",
    abstract = "While human evaluation remains best practice for accurately judging the faithfulness of automatically-generated summaries, few solutions exist to address the increased difficulty and workload when evaluating long-form summaries. Through a survey of 162 papers on long-form summarization, we first shed light on current human evaluation practices surrounding long-form summaries. We find that 73{\%} of these papers do not perform any human evaluation on model-generated summaries, while other works face new difficulties that manifest when dealing with long documents (e.g., low inter-annotator agreement). Motivated by our survey, we present LongEval, a set of guidelines for human evaluation of faithfulness in long-form summaries that addresses the following challenges: (1) How can we achieve high inter-annotator agreement on faithfulness scores? (2) How can we minimize annotator workload while maintaining accurate faithfulness scores? and (3) Do humans benefit from automated alignment between summary and source snippets? We deploy LongEval in annotation studies on two long-form summarization datasets in different domains (SQuALITY and PubMed), and we find that switching to a finer granularity of judgment (e.g., clause-level) reduces inter-annotator variance in faithfulness scores (e.g., std-dev from 18.5 to 6.8). We also show that scores from a partial annotation of fine-grained units highly correlates with scores from a full annotation workload (0.89 Kendall{'}s tau using 50{\%} judgements). We release our human judgments, annotation templates, and software as a Python library for future research.",
    bibtex_show = {true},
    arxiv = {2301.13298},
    acl = {2023.eacl-main.121},
    pdf={longeval-guidelines-for-human-evaluation-of-faithfulness-in-long-form-summarization.pdf},
    preview={longeval-guidelines-for-human-evaluation-of-faithfulness-in-long-form-summarization.png}
}

@article{Shen2023BeyondSD,
  title={Beyond Summarization: Designing AI Support for Real-World Expository Writing Tasks},
  author={Zejiang Shen and Tal August and Pao Siangliulue and Kyle Lo and Jonathan Bragg and Jeff Hammerbacher and Doug Downey and Joseph Chee Chang and David Sontag},
  booktitle={Intelligent and Interactive Writing Assistants (In2Writing) Workshop},
  year={2023},
  month={apr},
  volume={abs/2304.02623},
  abstract={Large language models have introduced exciting new opportunities and challenges in designing and developing new AI-assisted writing support tools. Recent work has shown that leveraging this new technology can transform writing in many scenarios such as ideation during creative writing, editing support, and summarization. However, AI-supported expository writing--including real-world tasks like scholars writing literature reviews or doctors writing progress notes--is relatively understudied. In this position paper, we argue that developing AI supports for expository writing has unique and exciting research challenges and can lead to high real-world impacts. We characterize expository writing as evidence-based and knowledge-generating: it contains summaries of external documents as well as new information or knowledge. It can be seen as the product of authors' sensemaking process over a set of source documents, and the interplay between reading, reflection, and writing opens up new opportunities for designing AI support. We sketch three components for AI support design and discuss considerations for future research.},
  bibtex_show={true},
  arxiv={2304.02623},
  pdf={beyond-summarization-designing-ai-support-for-real-world-expository-writing-tasks.pdf},
  preview={beyond-summarization-designing-ai-support-for-real-world-expository-writing-tasks.png}
}

@inproceedings{10.1145/3544548.3580847,
author = {Chang, Joseph Chee and Zhang, Amy X. and Bragg, Jonathan and Head, Andrew and Lo, Kyle and Downey, Doug and Weld, Daniel S.},
title = {CiteSee: Augmenting Citations in Scientific Papers with Persistent and Personalized Historical Context},
year = {2023},
month={apr},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580847},
doi = {10.1145/3544548.3580847},
abstract = {When reading a scholarly article, inline citations help researchers contextualize the current article and discover relevant prior work. However, it can be challenging to prioritize and make sense of the hundreds of citations encountered during literature reviews. This paper introduces CiteSee, a paper reading tool that leverages a user’s publishing, reading, and saving activities to provide personalized visual augmentations and context around citations. First, CiteSee connects the current paper to familiar contexts by surfacing known citations a user had cited or opened. Second, CiteSee helps users prioritize their exploration by highlighting relevant but unknown citations based on saving and reading history. We conducted a lab study that suggests CiteSee is significantly more effective for paper discovery than three baselines. A field deployment study shows CiteSee helps participants keep track of their explorations and leads to better situational awareness and increased paper discovery via inline citation when conducting real-world literature reviews.},
booktitle = {CHI},
articleno = {737},
numpages = {15},
keywords = {scientific papers, reading interfaces, personalization},
location = {Hamburg, Germany},
series = {CHI '23},
  bibtex_show={true},
  arxiv={2302.07302},
  acm={10.1145/3544548.3580847},
  pdf={citesee-augmenting-citations-in-scientific-papers-with-persistent-and-personalized-historical-context.pdf},
  preview={citesee-augmenting-citations-in-scientific-papers-with-persistent-and-personalized-historical-context.png}
}

@article{10.1145/3589955,
author = {August, Tal and Wang, Lucy Lu and Bragg, Jonathan and Hearst, Marti A. and Head, Andrew and Lo, Kyle},
title = {Paper Plain: Making Medical Research Papers Approachable to Healthcare Consumers with Natural Language Processing},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1073-0516},
url = {https://doi.org/10.1145/3589955},
doi = {10.1145/3589955},
abstract = {When seeking information not covered in patient-friendly documents, healthcare consumers may turn to the research literature. Reading medical papers, however, can be a challenging experience. To improve access to medical papers, we explore four features enabled by natural language processing: definitions of unfamiliar terms, in-situ plain language section summaries, a collection of key questions that guides readers to answering passages, and plain language summaries of those passages. We embody these features into a prototype system, Paper Plain. We evaluate Paper Plain, finding that participants who used the prototype system had an easier time reading research papers without a loss in paper comprehension compared to those who used a typical PDF reader. Altogether, the study results suggest that guiding readers to relevant passages and providing plain language summaries alongside the original paper content can make reading medical papers easier and give readers more confidence to approach these papers.},
journal = {ACM Transactions of Computer-Human Interaction (TOCHI)},
month = {apr},
selected={true},
bibtex_show={true},
arxiv={2203.00130},
acm={10.1145/3589955},
keywords = {augmented reading; plain language summaries; healthcare consumers},
pdf={paper-plain-making-medical-research-papers-approachable-to-healthcare-consumers-with-natural-language-processing.pdf},
preview={paper-plain-making-medical-research-papers-approachable-to-healthcare-consumers-with-natural-language-processing.png}
}

@article{Lo2023TheSR,
  title={The Semantic Reader Project: Augmenting Scholarly Documents through AI-Powered Interactive Reading Interfaces},
  author={Kyle Lo and Joseph Chee Chang and Andrew Head and Jonathan Bragg and Amy X. Zhang and Cassidy Trier and Chloe Anastasiades and Tal August and Russell Authur and Danielle Bragg and Erin Bransom and Isabel Cachola and Stefan Candra and Yoganand Chandrasekhar and Yen-Sung Chen and Evie (Yu-Yen) Cheng and Yvonne Chou and Doug Downey and Rob Evans and Raymond Fok and F.Q. Hu and Regan Huff and Dongyeop Kang and Tae Soo Kim and Rodney Michael Kinney and Aniket Kittur and Hyeonsu B Kang and Egor Klevak and Bailey Kuehl and Michael Langan and Matt Latzke and Jaron Lochner and Kelsey MacMillan and Eric Marsh and Tyler Murray and Aakanksha Naik and Ngoc-Uyen Nguyen and Srishti Palani and Soya Park and Caroline Paulic and Napol Rachatasumrit and Smita Rao and Paul L Sayre and Zejiang Shen and Pao Siangliulue and Luca Soldaini and Huy Tran and Madeleine van Zuylen and Lucy Lu Wang and Christopher Wilhelm and Caroline M Wu and Jiangjiang Yang and Angele Zamarron and Marti A. Hearst and Daniel S. Weld},
  journal={ArXiv},
  year={2023},
  month={mar},
  day={23},
  volume={abs/2303.14334},
  selected={true},
  abstract={Scholarly publications are key to the transfer of knowledge from scholars to others. However, research papers are information-dense, and as the volume of the scientific literature grows, the need for new technology to support the reading process grows. In contrast to the process of finding papers, which has been transformed by Internet technology, the experience of reading research papers has changed little in decades. The PDF format for sharing research papers is widely used due to its portability, but it has significant downsides including: static content, poor accessibility for low-vision readers, and difficulty reading on mobile devices. This paper explores the question "Can recent advances in AI and HCI power intelligent, interactive, and accessible reading interfaces -- even for legacy PDFs?" We describe the Semantic Reader Project, a collaborative effort across multiple institutions to explore automatic creation of dynamic reading interfaces for research papers. Through this project, we've developed ten research prototype interfaces and conducted usability studies with more than 300 participants and real-world users showing improved reading experiences for scholars. We've also released a production reading interface for research papers that will incorporate the best features as they mature. We structure this paper around challenges scholars and the public face when reading research papers -- Discovery, Efficiency, Comprehension, Synthesis, and Accessibility -- and present an overview of our progress and remaining open challenges.},
  bibtex_show={true},
  arxiv={2303.14334},
  pdf={the-semantic-reader-project-augmenting-scholarly-documents-through-ai-powered-interactive-reading-interfaces.pdf},
  preview={the-semantic-reader-project-augmenting-scholarly-documents-through-ai-powered-interactive-reading-interfaces.png}
}

@inproceedings{10.1145/3581641.3584034,
author = {Fok, Raymond and Kambhamettu, Hita and Soldaini, Luca and Bragg, Jonathan and Lo, Kyle and Hearst, Marti and Head, Andrew and Weld, Daniel S},
title = {Scim: Intelligent Skimming Support for Scientific Papers},
year = {2023},
month={mar},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584034},
doi = {10.1145/3581641.3584034},
abstract = {Scholars need to keep up with an exponentially increasing flood of scientific papers. To aid this challenge, we introduce Scim, a novel intelligent interface that helps experienced researchers skim – or rapidly review – a paper to attain a cursory understanding of its contents. Scim supports the skimming process by highlighting salient paper contents in order to direct a reader’s attention. The system’s highlights are faceted by content type, evenly distributed across a paper, and have a density configurable by readers at both the global and local level. We evaluate Scim with both an in-lab usability study and a longitudinal diary study, revealing how its highlights facilitate the more efficient construction of a conceptualization of a paper. We conclude by discussing design considerations and tensions for the design of future intelligent skimming tools.},
booktitle = {IUI},
pages = {476–490},
numpages = {15},
keywords = {scientific papers, Intelligent reading interfaces, highlights, skimming},
location = {Sydney, NSW, Australia},
series = {IUI '23},
bibtex_show={true},
arxiv={2205.04561},
acm={10.1145/3581641.3584034},
pdf={scim-intelligent-skimming-support-for-scientific-papers.pdf},
preview={scim-intelligent-skimming-support-for-scientific-papers.png}
}

@article{Kinney2023TheSS,
  title={The Semantic Scholar Open Data Platform},
  author={Rodney Michael Kinney and Chloe Anastasiades and Russell Authur and Iz Beltagy and Jonathan Bragg and Alexandra Buraczynski and Isabel Cachola and Stefan Candra and Yoganand Chandrasekhar and Arman Cohan and Miles Crawford and Doug Downey and Jason Dunkelberger and Oren Etzioni and Rob Evans and Sergey Feldman and Joseph Gorney and David W. Graham and F.Q. Hu and Regan Huff and Daniel King and Sebastian Kohlmeier and Bailey Kuehl and Michael Langan and Daniel Lin and Haokun Liu and Kyle Lo and Jaron Lochner and Kelsey MacMillan and Tyler Murray and Christopher Newell and Smita Rao and Shaurya Rohatgi and Paul L Sayre and Zejiang Shen and Amanpreet Singh and Luca Soldaini and Shivashankar Subramanian and A. Tanaka and Alex D Wade and Linda M. Wagner and Lucy Lu Wang and Christopher Wilhelm and Caroline Wu and Jiangjiang Yang and Angele Zamarron and Madeleine van Zuylen and Daniel S. Weld},
  journal={ArXiv},
  year={2023},
  month={jan},
  day={24},
  volume={abs/2301.10140},
  abstract={The volume of scientific output is creating an urgent need for automated tools to help scientists keep up with developments in their field. Semantic Scholar (S2) is an open data platform and website aimed at accelerating science by helping scholars discover and understand scientific literature. We combine public and proprietary data sources using state-of-the-art techniques for scholarly PDF content extraction and automatic knowledge graph construction to build the Semantic Scholar Academic Graph, the largest open scientific literature graph to-date, with 200M+ papers, 80M+ authors, 550M+ paper-authorship edges, and 2.4B+ citation edges. The graph includes advanced semantic features such as structurally parsed text, natural language summaries, and vector embeddings. In this paper, we describe the components of the S2 data processing pipeline and the associated APIs offered by the platform. We will update this living document to reflect changes as we add new data offerings and improve existing services.},
  bibtex_show={true},
  arxiv={2301.10140},
  pdf={the-semantic-scholar-open-data-platform.pdf},
  preview={the-semantic-scholar-open-data-platform.png}
}

@article{Giorgi2022ExploringTC,
  title={Exploring the Challenges of Open Domain Multi-Document Summarization},
  author={John Giorgi and Luca Soldaini and Bo Wang and Gary Bader and Kyle Lo and Lucy Lu Wang and Arman Cohan},
  journal={ArXiv},
  year={2022},
  month={dec},
  day={20},
  volume={abs/2212.10526},
  abstract={Multi-document summarization (MDS) has traditionally been studied assuming a set of ground-truth topic-related input documents is provided. In practice, the input document set is unlikely to be available a priori and would need to be retrieved based on an information need, a setting we call open-domain MDS. We experiment with current state-of-the-art retrieval and summarization models on several popular MDS datasets extended to the open-domain setting. We find that existing summarizers suffer large reductions in performance when applied as-is to this more realistic task, though training summarizers with retrieved inputs can reduce their sensitivity retrieval errors. To further probe these findings, we conduct perturbation experiments on summarizer inputs to study the impact of different types of document retrieval errors. Based on our results, we provide practical guidelines to help facilitate a shift to open-domain MDS. We release our code and experimental results alongside all data or model artifacts created during our investigation.},
  bibtex_show={true},
  arxiv={2212.10526},
  pdf={exploring-the-challenges-of-open-domain-multi-document-summarization.pdf},
  preview={exploring-the-challenges-of-open-domain-multi-document-summarization.png}
}

@inproceedings{wadden-etal-2022-scifact,
    title = "{S}ci{F}act-Open: Towards open-domain scientific claim verification",
    author = "Wadden, David  and
      Lo, Kyle  and
      Kuehl, Bailey  and
      Cohan, Arman  and
      Beltagy, Iz  and
      Wang, Lucy Lu  and
      Hajishirzi, Hannaneh",
    booktitle = "Findings of EMNLP",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.347",
    pages = "4719--4734",
    abstract = "While research on scientific claim verification has led to the development of powerful systems that appear to approach human performance, these approaches have yet to be tested in a realistic setting against large corpora of scientific literature. Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFact-Open, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts. Drawing upon pooling techniques from information retrieval, we collect evidence for scientific claims by pooling and annotating the top predictions of four state-of-the-art scientific claim verification models. We find that systems developed on smaller corpora struggle to generalize to SciFact-Open, exhibiting performance drops of at least 15 F1. In addition, analysis of the evidence in SciFact-Open reveals interesting phenomena likely to appear when claim verification systems are deployed in practice, e.g., cases where the evidence supports only a special case of the claim. Our dataset is available at https://github.com/dwadden/scifact-open.",
    bibtex_show={true},
    acl={2022.findings-emnlp.347},
    pdf={scifact-open-towards-open-domain-scientific-claim-verification.pdf},
    preview={scifact-open-towards-open-domain-scientific-claim-verification.png}
}

@article{Scao2022BLOOMA1,
  title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author={Teven Le Scao and Angela Fan and Christopher Akiki and Elizabeth-Jane Pavlick and Suzana Ili'c and Daniel Hesslow and Roman Castagn'e and Alexandra Sasha Luccioni and Franccois Yvon and Matthias Gall{\'e} and Jonathan Tow and Alexander M. Rush and Stella Rose Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Beno{\^i}t Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo Laurenccon and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and Aaron Gokaslan and Adi Simhi and Aitor Soroa Etxabe and Alham Fikri Aji and Amit Alfassy and Anna Rogers and Ariel Kreisberg Nitzav and Canwen Xu and Chenghao Mou and Chris C. Emezue and Christopher Klamm and Colin Leong and Daniel Alexander van Strien and David Ifeoluwa Adelani and Dragomir R. Radev and Eduardo G. Ponferrada and Efrat Levkovizh and Ethan Kim and Eyal Bar Natan and Francesco De Toni and G{\'e}rard Dupont and Germ{\'a}n Kruszewski and Giada Pistilli and Hady ElSahar and Hamza Benyamina and Hieu Tran and Ian Yu and Idris Abdulmumin and Isaac Johnson and Itziar Gonzalez-Dios and Javier de la Rosa and Jenny Chim and Jesse Dodge and Jian Zhu and Jonathan Chang and Jorg Frohberg and Josephine L. Tobing and Joydeep Bhattacharjee and Khalid Almubarak and Kimbo Chen and Kyle Lo and Leandro von Werra and Leon Weber and Long Phan and Loubna Ben Allal and Ludovic Tanguy and Manan Dey and Manuel Romero Mu{\~n}oz and Maraim Masoud and Mar'ia Grandury and Mario vSavsko and Max Huang and Maximin Coavoux and Mayank Singh and Mike Tian-Jian Jiang and Minh Chien Vu and Mohammad Ali Jauhar and Mustafa Ghaleb and Nishant Subramani and Nora Kassner and Nurulaqilla Khamis and Olivier Nguyen and Omar Espejel and Ona de Gibert and Paulo Villegas and Peter Henderson and Pierre Colombo and Priscilla Amuok and Quentin Lhoest and Rheza Harliman and Rishi Bommasani and Roberto L'opez and R. Ribeiro and Salomey Osei and Sampo Pyysalo and Sebastian Nagel and Shamik Bose and Shamsuddeen Hassan Muhammad and Shanya Sharma and S. Longpre and Somaieh Nikpoor and Stanislav Silberberg and Suhas Pai and Sydney Zink and Tiago Timponi Torrent and Timo Schick and Tristan Thrush and Valentin Danchev and Vassilina Nikoulina and Veronika Laippala and Violette Lepercq and Vrinda Prabhu and Zaid Alyafeai and Zeerak Talat and Arun Raja and Benjamin Heinzerling and Chenglei Si and Elizabeth Salesky and Sabrina J. Mielke and Wilson Y. Lee and Abheesht Sharma and Andrea Santilli and Antoine Chaffin and Arnaud Stiegler and Debajyoti Datta and Eliza Szczechla and Gunjan Chhablani and Han Wang and Harshit Pandey and Hendrik Strobelt and Jason Alan Fries and Jos Rozen and Leo Gao and Lintang Sutawika and M Saiful Bari and Maged S. Al-shaibani and Matteo Manica and Nihal V. Nayak and Ryan Teehan and Samuel Albanie and Sheng Shen and Srulik Ben-David and Stephen H. Bach and Taewoon Kim and Tali Bers and Thibault F{\'e}vry and Trishala Neeraj and Urmish Thakker and Vikas Raunak and Xiang Tang and Zheng Xin Yong and Zhiqing Sun and Shaked Brody and Y Uri and Hadar Tojarieh and Adam Roberts and Hyung Won Chung and Jaesung Tae and Jason Phang and Ofir Press and Conglong Li and Deepak Narayanan and Hatim Bourfoune and Jared Casper and Jeff Rasley and Max Ryabinin and Mayank Mishra and Minjia Zhang and Mohammad Shoeybi and Myriam Peyrounette and Nicolas Patry and Nouamane Tazi and Omar Sanseviero and Patrick von Platen and Pierre Cornette and Pierre Franccois Lavall'ee and R{\'e}mi Lacroix and Samyam Rajbhandari and Sanchit Gandhi and Shaden Smith and St{\'e}phane Requena and Suraj Patil and Tim Dettmers and Ahmed Baruwa and Amanpreet Singh and Anastasia Cheveleva and Anne-Laure Ligozat and Arjun Subramonian and Aur'elie N'ev'eol and Charles Lovering and Daniel H Garrette and Deepak R. Tunuguntla and Ehud Reiter and Ekaterina Taktasheva and Ekaterina Voloshina and Eli Bogdanov and Genta Indra Winata and Hailey Schoelkopf and Jan-Christoph Kalo and Jekaterina Novikova and Jessica Zosa Forde and Jordan Clive and Jungo Kasai and Ken Kawamura and Liam Hazan and Marine Carpuat and Miruna Clinciu and Najoung Kim and Newton Cheng and Oleg Serikov and Omer Antverg and Oskar van der Wal and Rui Zhang and Ruochen Zhang and Sebastian Gehrmann and S. Osher Pais and Tatiana Shavrina and Thomas Scialom and Tian Yun and Tomasz Limisiewicz and Verena Rieser and Vitaly Protasov and Vladislav Mikhailov and Yada Pruksachatkun and Yonatan Belinkov and Zachary Bamberger and Zdenvek Kasner and Alice Rueda and Amanda Pestana and Amir Feizpour and Ammar Khan and Amy Faranak and Ananda Santa Rosa Santos and Anthony Hevia and Antigona Unldreaj and Arash Aghagol and Arezoo Abdollahi and Aycha Tammour and Azadeh HajiHosseini and Bahareh Behroozi and Benjamin Olusola Ajibade and Bharat Kumar Saxena and Carlos Mu{\~n}oz Ferrandis and Danish Contractor and David M. Lansky and Davis David and Douwe Kiela and Duong Anh Nguyen and Edward Tan and Emily Baylor and Ezinwanne Ozoani and Fatim T Mirza and Frankline Ononiwu and Habib Rezanejad and H.A. Jones and Indrani Bhattacharya and Irene Solaiman and Irina Sedenko and Isar Nejadgholi and J. Lawrence Passmore and Joshua Seltzer and Julio Bonis Sanz and Karen Fort and L{\'i}via Macedo Dutra and Mairon Samagaio and Maraim Elbadri and Margot Mieskes and Marissa Gerchick and Martha Akinlolu and Michael McKenna and Mike Qiu and M. K. K. Ghauri and Mykola Burynok and Nafis Abrar and Nazneen Rajani and Nour Elkott and Nourhan Fahmy and Olanrewaju Modupe Samuel and Ran An and R. P. Kromann and Ryan Hao and Samira Alizadeh and Sarmad Shubber and Silas L. Wang and Sourav Roy and Sylvain Viguier and Thanh-Cong Le and Tobi Oyebade and Trieu Nguyen Hai Le and Yoyo Yang and Zachary Kyle Nguyen and Abhinav Ramesh Kashyap and Alfredo Palasciano and Alison Callahan and Anima Shukla and Antonio Miranda-Escalada and Ayush Kumar Singh and Benjamin Beilharz and Bo Wang and Caio Matheus Fonseca de Brito and Chenxi Zhou and Chirag Jain and Chuxin Xu and Cl{\'e}mentine Fourrier and Daniel Le'on Perin'an and Daniel Molano and Dian Yu and Enrique Manjavacas and Fabio Barth and Florian Fuhrimann and Gabriel Altay and Giyaseddin Bayrak and Gully A. Burns and Helena U. Vrabec and Iman I.B. Bello and Isha Dash and Ji Soo Kang and John Giorgi and Jonas Golde and Jose David Posada and Karthi Sivaraman and Lokesh Bulchandani and Lu Liu and Luisa Shinzato and Madeleine Hahn de Bykhovetz and Maiko Takeuchi and Marc P{\`a}mies and Mar{\'i}a Andrea Castillo and Marianna Nezhurina and Mario Sanger and Matthias Samwald and Michael Cullan and Michael Weinberg and M Wolf and Mina Mihaljcic and Minna Liu and Moritz Freidank and Myungsun Kang and Natasha Seelam and Nathan Dahlberg and Nicholas Michio Broad and Nikolaus Muellner and Pascale Fung and Patricia Haller and R. Chandrasekhar and R. Eisenberg and Robert Martin and Rodrigo L. Canalli and Rosaline Su and Ruisi Su and Samuel Cahyawijaya and Samuele Garda and Shlok S Deshmukh and Shubhanshu Mishra and Sid Kiblawi and Simon Ott and Sinee Sang-aroonsiri and Srishti Kumar and Stefan Schweter and Sushil Pratap Bharati and T. A. Laud and Th'eo Gigant and Tomoya Kainuma and Wojciech Kusa and Yanis Labrak and Yashasvi Bajaj and Y. Venkatraman and Yifan Xu and Ying Xu and Yun-chao Xu and Zhee Xao Tan and Zhongli Xie and Zifan Ye and Mathilde Bras and Younes Belkada and Thomas Wolf},
  journal={ArXiv},
  year={2022},
  month={nov},
  day={9},
  volume={abs/2211.05100},
  abstract={Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
  bibtex_show={true},
  arxiv={2211.05100},
pdf={bloom-a-176b-parameter-open-access-multilingual-language-model.pdf},
preview={bloom-a-176b-parameter-open-access-multilingual-language-model.png}
}

@inproceedings{shen2022multilexsum,
title={Multi-LexSum: Real-world Summaries of Civil Rights Lawsuits at Multiple Granularities},
author={Zejiang Shen and Kyle Lo and Lauren Yu and Nathan Dahlberg and Margo Schlanger and Doug Downey},
booktitle={NeurIPS (Datasets and Benchmarks)},
year={2022},
month={nov},
day={28},
url={https://openreview.net/forum?id=z1d8fUiS8Cr},
selected={true},
abstract={With the advent of large language models, methods for abstractive summarization have made great strides, creating potential for use in applications to aid knowledge workers processing unwieldy document collections. One such setting is the Civil Rights Litigation Clearinghouse (CRLC, https://clearinghouse.net), which posts information about large-scale civil rights lawsuits, serving lawyers, scholars, and the general public. Today, summarization in the CRLC requires extensive training of lawyers and law students who spend hours per case understanding multiple relevant documents in order to produce high-quality summaries of key events and outcomes. Motivated by this ongoing real-world summarization effort, we introduce Multi-LexSum, a collection of 9,280 expert-authored summaries drawn from ongoing CRLC writing. Multi-LexSum presents a challenging multi-document summarization task given the length of the source documents, often exceeding two hundred pages per case. Furthermore, Multi-LexSum is distinct from other datasets in its multiple target summaries, each at a different granularity (ranging from one-sentence "extreme" summaries to multi-paragraph narrations of over five hundred words). We present extensive analysis demonstrating that despite the high-quality summaries in the training data (adhering to strict content and style guidelines), state-of-the-art summarization models perform poorly on this task. We release Multi-LexSum for further summarization research and to facilitate the development of applications to assist in the CRLC's mission at https://multilexsum.github.io.},
bibtex_show={true},
openreview={z1d8fUiS8Cr},
pdf={multi-lexsum-real-world-summaries-of-civil-rights-lawsuits-at-multiple-granularities.pdf},
preview={multi-lexsum-real-world-summaries-of-civil-rights-lawsuits-at-multiple-granularities.png}
}

@inproceedings{cohan-etal-2022-overview,
    title = "Overview of the Third Workshop on Scholarly Document Processing",
    author = "Cohan, Arman  and
      Feigenblat, Guy  and
      Freitag, Dayne  and
      Ghosal, Tirthankar  and
      Herrmannova, Drahomira  and
      Knoth, Petr  and
      Lo, Kyle  and
      Mayr, Philipp  and
      Shmueli-Scheuer, Michal  and
      de Waard, Anita  and
      Wang, Lucy Lu",
    booktitle = "Scholarly Document Processing (SDP) Workshop",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.sdp-1.1",
    pages = "1--6",
    abstract = "With the ever-increasing pace of research and high volume of scholarly communication, scholars face a daunting task. Not only must they keep up with the growing literature in their own and related fields, scholars increasingly also need to rebut pseudo-science and disinformation. These needs have motivated an increasing focus on computational methods for enhancing search, summarization, and analysis of scholarly documents. However, the various strands of research on scholarly document processing remain fragmented. To reach out to the broader NLP and AI/ML community, pool distributed efforts in this area, and enable shared access to published research, we held the 3rd Workshop on Scholarly Document Processing (SDP) at COLING as a hybrid event (https://sdproc.org/2022/). The SDP workshop consisted of a research track, three invited talks and five Shared Tasks: 1) MSLR22: Multi-Document Summarization for Literature Reviews, 2) DAGPap22: Detecting automatically generated scientific papers, 3) SV-Ident 2022: Survey Variable Identification in Social Science Publications, 4) SKGG: Scholarly Knowledge Graph Generation, 5) MuP 2022: Multi Perspective Scientific Document Summarization. The program was geared towards NLP, information retrieval, and data mining for scholarly documents, with an emphasis on identifying and providing solutions to open challenges.",
    bibtex_show={true},
    acl={2022.sdp-1.1},
    pdf={overview-of-the-third-workshop-on-scholarly-document-processing.pdf},
    preview={overview-of-the-third-workshop-on-scholarly-document-processing.png}
}

@inproceedings{wadden-etal-2022-multivers,
    title = "{M}ulti{V}er{S}: Improving scientific claim verification with weak supervision and full-document context",
    author = "Wadden, David  and
      Lo, Kyle  and
      Wang, Lucy Lu  and
      Cohan, Arman  and
      Beltagy, Iz  and
      Hajishirzi, Hannaneh",
    booktitle = "Findings of NAACL",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.6",
    doi = "10.18653/v1/2022.findings-naacl.6",
    pages = "61--76",
    abstract = "The scientific claim verification task requires an NLP system to label scientific documents which Support or Refute an input claim, and to select evidentiary sentences (or rationales) justifying each predicted label. In this work, we present MultiVerS, which predicts a fact-checking label and identifies rationales in a multitask fashion based on a shared encoding of the claim and full document context. This approach accomplishes two key modeling goals. First, it ensures that all relevant contextual information is incorporated into each labeling decision. Second, it enables the model to learn from instances annotated with a document-level fact-checking label, but lacking sentence-level rationales. This allows MultiVerS to perform weakly-supervised domain adaptation by training on scientific documents labeled using high-precision heuristics. Our approach outperforms two competitive baselines on three scientific claim verification datasets, with particularly strong performance in zero / few-shot domain adaptation experiments. Our code and data are available at https://github.com/dwadden/multivers.",
  bibtex_show={true},
  acl={2022.findings-naacl.6},
  pdf={multivers-improving-scientific-claim-verification-with-weak-supervision-and-full-document-context.pdf},
  preview={multivers-improving-scientific-claim-verification-with-weak-supervision-and-full-document-context.png}
}

@inproceedings{lauscher-etal-2022-multicite,
    title = "{M}ulti{C}ite: Modeling realistic citations requires moving beyond the single-sentence single-label setting",
    author = "Lauscher, Anne  and
      Ko, Brandon  and
      Kuehl, Bailey  and
      Johnson, Sophie  and
      Cohan, Arman  and
      Jurgens, David  and
      Lo, Kyle",
    booktitle = "NAACL",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.137",
    doi = "10.18653/v1/2022.naacl-main.137",
    pages = "1875--1889",
    abstract = "Citation context analysis (CCA) is an important task in natural language processing that studies how and why scholars discuss each others{'} work. Despite decades of study, computational methods for CCA have largely relied on overly-simplistic assumptions of how authors cite, which ignore several important phenomena. For instance, scholarly papers often contain rich discussions of cited work that span multiple sentences and express multiple intents concurrently. Yet, recent work in CCA is often approached as a single-sentence, single-label classification task, and thus many datasets used to develop modern computational approaches fail to capture this interesting discourse. To address this research gap, we highlight three understudied phenomena for CCA and release MULTICITE, a new dataset of 12.6K citation contexts from 1.2K computational linguistics papers that fully models these phenomena. Not only is it the largest collection of expert-annotated citation contexts to-date, MULTICITE contains multi-sentence, multi-label citation contexts annotated through-out entire full paper texts. We demonstrate how MULTICITE can enable the development of new computational methods on three important CCA tasks. We release our code and dataset at https://github.com/allenai/multicite.",
    bibtex_show={true},
    acl={2022.naacl-main.137},
pdf={multicite-modeling-realistic-citations-requires-moving-beyond-the-single-sentence-single-label-setting.pdf},
preview={multicite-modeling-realistic-citations-requires-moving-beyond-the-single-sentence-single-label-setting.png}
}

@article{Goodwin2022,
author={Goodwin, Travis R.
and Demner-Fushman, Dina
and Lo, Kyle
and Wang, Lucy Lu
and Dang, Hoa T.
and Soboroff, Ian M.},
title={Automatic question answering for multiple stakeholders, the epidemic question answering dataset},
journal={Scientific Data},
year={2022},
month={Jul},
day={21},
volume={9},
number={1},
pages={432},
abstract={One of the effects of COVID-19 pandemic is a rapidly growing and changing stream of publications to inform clinicians, researchers, policy makers, and patients about the health, socio-economic, and cultural consequences of the pandemic. Managing this information stream manually is not feasible. Automatic Question Answering can quickly bring the most salient points to the user's attention. Leveraging a collection of scientific articles, government websites, relevant news articles, curated social media posts, and questions asked by researchers, clinicians, and the general public, we developed a dataset to explore automatic Question Answering for multiple stakeholders. Analysis of questions asked by various stakeholders shows that while information needs of experts and the public may overlap, satisfactory answers to these questions often originate from different information sources or benefit from different approaches to answer generation. We believe that this dataset has the potential to support the development of question answering systems not only for epidemic questions, but for other domains with varying expertise such as legal or finance.},
issn={2052-4463},
doi={10.1038/s41597-022-01533-w},
url={https://doi.org/10.1038/s41597-022-01533-w},
bibtex_show={true},
nature={s41597-022-01533-w},
pdf={automatic-question-answering-for-multiple-stakeholders-the-epidemic-question-answering-dataset.pdf},
preview={automatic-question-answering-for-multiple-stakeholders-the-epidemic-question-answering-dataset.png}
}

@inproceedings{10.1145/3531146.3534637,
author = {Jernite, Yacine and Nguyen, Huu and Biderman, Stella and Rogers, Anna and Masoud, Maraim and Danchev, Valentin and Tan, Samson and Luccioni, Alexandra Sasha and Subramani, Nishant and Johnson, Isaac and Dupont, Gerard and Dodge, Jesse and Lo, Kyle and Talat, Zeerak and Radev, Dragomir and Gokaslan, Aaron and Nikpoor, Somaieh and Henderson, Peter and Bommasani, Rishi and Mitchell, Margaret},
title = {Data Governance in the Age of Large-Scale Data-Driven Language Technology},
year = {2022},
month = {jun},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534637},
doi = {10.1145/3531146.3534637},
abstract = {The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work.},
booktitle = {FAccT},
pages = {2206–2222},
numpages = {17},
keywords = {language data, datasets, technology governance, data rights},
location = {Seoul, Republic of Korea},
series = {FAccT '22},
bibtex_show={true},
arxiv={2206.03216},
acm={10.1145/3531146.3534637},
pdf={data-governance-in-the-age-of-large-scale-data-driven-language-technology.pdf},
preview={data-governance-in-the-age-of-large-scale-data-driven-language-technology.png}
}

@inproceedings{bigsciencerootscorpus,
title={The BigScience {ROOTS} Corpus: A 1.6{TB} Composite Multilingual Dataset},
author={Hugo Lauren{\c{c}}on and Lucile Saulnier and Thomas Wang and Christopher Akiki and Albert Villanova del Moral and Teven Le Scao and Leandro Von Werra and Chenghao Mou and Eduardo Gonz{\'a}lez Ponferrada and Huu Nguyen and J{\"o}rg Frohberg and Mario {\v{S}}a{\v{s}}ko and Quentin Lhoest and Angelina McMillan-Major and G{\'e}rard Dupont and Stella Biderman and Anna Rogers and Loubna Ben allal and Francesco De Toni and Giada Pistilli and Olivier Nguyen and Somaieh Nikpoor and Maraim Masoud and Pierre Colombo and Javier de la Rosa and Paulo Villegas and Tristan Thrush and Shayne Longpre and Sebastian Nagel and Leon Weber and Manuel Romero Mu{\~n}oz and Jian Zhu and Daniel Van Strien and Zaid Alyafeai and Khalid Almubarak and Vu Minh Chien and Itziar Gonzalez-Dios and Aitor Soroa and Kyle Lo and Manan Dey and Pedro Ortiz Suarez and Aaron Gokaslan and Shamik Bose and David Ifeoluwa Adelani and Long Phan and Hieu Tran and Ian Yu and Suhas Pai and Jenny Chim and Violette Lepercq and Suzana Ilic and Margaret Mitchell and Sasha Luccioni and Yacine Jernite},
booktitle={NeurIPS (Datasets and Benchmarks)},
year={2022},
month={may},
url={https://openreview.net/forum?id=UoEw6KigkUn},
abstract={As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.},
openreview={UoEw6KigkUn},
bibtex_show={true},
pdf={bigscience-roots-corpus-a-1-6tb-composite-multilingual-dataset.pdf},
preview={bigscience-roots-corpus-a-1-6tb-composite-multilingual-dataset.png}
}

@inproceedings{wright-etal-2022-generating,
    title = "Generating Scientific Claims for Zero-Shot Scientific Fact Checking",
    author = "Wright, Dustin  and
      Wadden, David  and
      Lo, Kyle  and
      Kuehl, Bailey  and
      Cohan, Arman  and
      Augenstein, Isabelle  and
      Wang, Lucy Lu",
    booktitle = "ACL",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.175",
    doi = "10.18653/v1/2022.acl-long.175",
    pages = "2448--2460",
    abstract = "Automated scientific fact checking is difficult due to the complexity of scientific language and a lack of significant amounts of training data, as annotation requires domain expertise. To address this challenge, we propose scientific claim generation, the task of generating one or more atomic and verifiable claims from scientific sentences, and demonstrate its usefulness in zero-shot fact checking for biomedical claims. We propose CLAIMGEN-BART, a new supervised method for generating claims supported by the literature, as well as KBIN, a novel method for generating claim negations. Additionally, we adapt an existing unsupervised entity-centric method of claim generation to biomedical claims, which we call CLAIMGEN-ENTITY. Experiments on zero-shot fact checking demonstrate that both CLAIMGEN-ENTITY and CLAIMGEN-BART, coupled with KBIN, achieve up to 90{\%} performance of fully supervised models trained on manually annotated claims and evidence. A rigorous evaluation study demonstrates significant improvement in generated claim and negation quality over existing baselines",
    bibtex_show={true},
    acl={2022.acl-long.175},
    pdf={generating-scientific-claims-for-zero-shot-scientific-fact-checking.pdf},
    preview={generating-scientific-claims-for-zero-shot-scientific-fact-checking.png}
}

@article{shen-etal-2022-vila,
    title = {{VILA}: Improving Structured Content Extraction from Scientific {PDF}s Using Visual Layout Groups},
    author = "Shen, Zejiang  and
      Lo, Kyle  and
      Wang, Lucy Lu  and
      Kuehl, Bailey  and
      Weld, Daniel S.  and
      Downey, Doug",
    journal = "Transactions of ACL (TACL)",
    volume = "10",
    year = "2022",
    month = "may",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.22",
    doi = "10.1162/tacl_a_00466",
    pages = "376--392",
    abstract = "Accurately extracting structured content from PDFs is a critical first step for NLP over scientific papers. Recent work has improved extraction accuracy by incorporating elementary layout information, for example, each token{'}s 2D position on the page, into language model pretraining. We introduce new methods that explicitly model VIsual LAyout (VILA) groups, that is, text lines or text blocks, to further improve performance. In our I-VILA approach, we show that simply inserting special tokens denoting layout group boundaries into model inputs can lead to a 1.9{\%} Macro F1 improvement in token classification. In the H-VILA approach, we show that hierarchical encoding of layout-groups can result in up to 47{\%} inference time reduction with less than 0.8{\%} Macro F1 loss. Unlike prior layout-aware approaches, our methods do not require expensive additional pretraining, only fine-tuning, which we show can reduce training cost by up to 95{\%}. Experiments are conducted on a newly curated evaluation suite, S2-VLUE, that unifies existing automatically labeled datasets and includes a new dataset of manual annotations covering diverse papers from 19 scientific disciplines. Pre-trained weights, benchmark datasets, and source code are available at https://github.com/allenai/VILA.",
    selected={true},
    bibtex_show={true},
    acl={2022.tacl-1.22},
    pdf={vila-improving-structured-content-extraction-from-scientific-pdfs-using-visual-layout-groups.pdf},
    preview={vila-improving-structured-content-extraction-from-scientific-pdfs-using-visual-layout-groups.png}
}

@article{Murthy2022ACCoRDAM,
  title={ACCoRD: A Multi-Document Approach to Generating Diverse Descriptions of Scientific Concepts},
  author={Sonia K. Murthy and Kyle Lo and Daniel King and Chandra Bhagavatula and Bailey Kuehl and Sophie Johnson and Jon Borchardt and Daniel S. Weld and Tom Hope and Doug Downey},
  journal={ArXiv},
  year={2022},
  month={may},
  day={14},
  volume={abs/2205.06982},
  abstract={Systems that can automatically define unfamiliar terms hold the promise of improving the accessibility of scientific texts, especially for readers who may lack prerequisite background knowledge. However, current systems assume a single "best" description per concept, which fails to account for the many potentially useful ways a concept can be described. We present ACCoRD, an end-to-end system tackling the novel task of generating sets of descriptions of scientific concepts. Our system takes advantage of the myriad ways a concept is mentioned across the scientific literature to produce distinct, diverse descriptions of target scientific concepts in terms of different reference concepts. To support research on the task, we release an expert-annotated resource, the ACCoRD corpus, which includes 1,275 labeled contexts and 1,787 hand-authored concept descriptions. We conduct a user study demonstrating that (1) users prefer descriptions produced by our end-to-end system, and (2) users prefer multiple descriptions to a single "best" description.},
  bibtex_show={true},
  arxiv={2205.06982},
  pdf={accord-a-multi-document-approach-to-generating-diverse-descriptions-of-scientific-concepts.pdf},
  preview={accord-a-multi-document-approach-to-generating-diverse-descriptions-of-scientific-concepts.png}
}

@inproceedings{10.1145/3491101.3519795,
author = {Radensky, Marissa and Downey, Doug and Lo, Kyle and Popovic, Zoran and Weld, Daniel S},
title = {Exploring the Role of Local and Global Explanations in Recommender Systems},
year = {2022},
month={apr},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3519795},
doi = {10.1145/3491101.3519795},
abstract = {Explanations are well-known to improve recommender systems’ transparency. These explanations may be local, explaining individual recommendations, or global, explaining the recommender model overall. Despite their widespread use, there has been little investigation into the relative benefits of the two explanation approaches. We conducted a 30-participant exploratory study and a 30-participant controlled user study with a research-paper recommender to analyze how providing local, global, or both explanations influences user understanding of system behavior. Our results provide evidence suggesting that both are more helpful than either alone for explaining how to improve recommendations, yet both appeared less helpful than global alone for efficiently identifying false positive and negative recommendations. However, we note that the two explanation approaches may be better compared in a higher-stakes or more opaque domain.},
booktitle = {CHI (Extended Abstracts)},
articleno = {290},
numpages = {7},
keywords = {explainable AI, human-AI interaction},
location = {New Orleans, LA, USA},
series = {CHI EA '22},
bibtex_show={true},
arxiv={2109.13301},
acm={10.1145/3491101.3519795},
pdf={exploring-the-role-of-local-and-global-explanations-in-recommender-systems.pdf},
preview={exploring-the-role-of-local-and-global-explanations-in-recommender-systems.png}
}

@article{10.1002/aaai.12038,
author = {Cafarella, Michael and Anderson, Michael and Beltagy, Iz and Cattan, Arie and Chasins, Sarah and Dagan, Ido and Downey, Doug and Etzioni, Oren and Feldman, Sergey and Gao, Tian and Hope, Tom and Huang, Kexin and Johnson, Sophie and King, Daniel and Lo, Kyle and Lou, Yuze and Shapiro, Matthew and Shen, Dinghao and Subramanian, Shivashankar and Wang, Lucy Lu and Wang, Yuning and Wang, Yitong and Weld, Daniel S. and Vo-Phamhi, Jenny and Zeng, Anna and Zou, Jiayun},
title = {Infrastructure for rapid open knowledge network development},
journal = {AI Magazine},
volume = {43},
number = {1},
pages = {59-68},
doi = {https://doi.org/10.1002/aaai.12038},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aaai.12038},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/aaai.12038},
abstract = {Abstract The past decade has witnessed a growth in the use of knowledge graph technologies for advanced data search, data integration, and query-answering applications. The leading example of a public, general-purpose open knowledge network (aka knowledge graph) is Wikidata, which has demonstrated remarkable advances in quality and coverage over this time. Proprietary knowledge graphs drive some of the leading applications of the day including, for example, Google Search, Alexa, Siri, and Cortana. Open Knowledge Networks are exciting: they promise the power of structured database-like queries with the potential for the wide coverage that is today only provided by the Web. With the current state of the art, building, using, and scaling large knowledge networks can still be frustratingly slow. This article describes a National Science Foundation Convergence Accelerator project to build a set of Knowledge Network Programming Infrastructure systems to address this issue.},
year = {2022},
month = {mar},
day = {31},
bibtex_show={true},
aaai={aimagazine/article/view/19126},
pdf={infrastructure-for-rapid-open-knowledge-network-development.pdf},
preview={infrastructure-for-rapid-open-knowledge-network-development.png}
}

@inproceedings{NEURIPS2021_8493eeac,
 author = {Bragg, Jonathan and Cohan, Arman and Lo, Kyle and Beltagy, Iz},
 booktitle = {NeurIPS},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {15787--15800},
 publisher = {Curran Associates, Inc.},
 title = {FLEX: Unifying Evaluation for Few-Shot NLP},
 url = {https://proceedings.neurips.cc/paper/2021/file/8493eeaccb772c0878f99d60a0bd2bb3-Paper.pdf},
 volume = {34},
 year = {2021},
 month = {dec},
 bibtex_show={true},
 abstract={Few-shot NLP research is highly active, yet conducted in disjoint research threads with evaluation suites that lack challenging-yet-realistic testing setups and fail to employ careful experimental design. Consequently, the community does not know which techniques perform best or even if they outperform simple baselines. In response, we formulate the FLEX Principles, a set of requirements and best practices for unified, rigorous, valid, and cost-sensitive few-shot NLP evaluation. These principles include Sample Size Design, a novel approach to benchmark design that optimizes statistical accuracy and precision while keeping evaluation costs manageable. Following the principles, we release the FLEX benchmark, which includes four few-shot transfer settings, zero-shot evaluation, and a public leaderboard that covers diverse NLP tasks. In addition, we present UniFew, a prompt-based model for few-shot learning that unifies pretraining and finetuning prompt formats, eschewing complex machinery of recent prompt-based approaches in adapting downstream task formats to language model pretraining objectives. We demonstrate that despite simplicity, UniFew achieves results competitive with both popular meta-learning and prompt-based approaches.},
 openreview={_WnGcwXLYOE},
 pdf={flex-unifying-evaluation-for-few-shot-nlp.pdf},
 preview={flex-unifying-evaluation-for-few-shot-nlp.png}
}


@inproceedings{luu-etal-2021-explaining,
    title = "Explaining Relationships Between Scientific Documents",
    author = "Luu, Kelvin  and
      Wu, Xinyi  and
      Koncel-Kedziorski, Rik  and
      Lo, Kyle  and
      Cachola, Isabel  and
      Smith, Noah A.",
    booktitle = "ACL",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.166",
    doi = "10.18653/v1/2021.acl-long.166",
    pages = "2130--2144",
    abstract = "We address the task of explaining relationships between two scientific documents using natural language text. This task requires modeling the complex content of long technical documents, deducing a relationship between these documents, and expressing the details of that relationship in text. In addition to the theoretical interest of this task, successful solutions can help improve researcher efficiency in search and review. In this paper we establish a dataset of 622K examples from 154K documents. We pretrain a large language model to serve as the foundation for autoregressive approaches to the task. We explore the impact of taking different views on the two documents, including the use of dense representations extracted with scientific IE systems. We provide extensive automatic and human evaluations which show the promise of such models, but make clear challenges for future work.",
    bibtex_show={true},
    acl={2021.acl-long.166},
pdf={explaining-relationships-between-scientific-documents.pdf},
preview={explaining-relationships-between-scientific-documents.png}
}

@inproceedings{dasigi-etal-2021-dataset,
    title = "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers",
    author = "Dasigi, Pradeep  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Cohan, Arman  and
      Smith, Noah A.  and
      Gardner, Matt",
    booktitle = "NAACL",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.365",
    doi = "10.18653/v1/2021.naacl-main.365",
    pages = "4599--4610",
    abstract = "Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present Qasper, a dataset of 5049 questions over 1585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.",
    selected={true},
    bibtex_show={true},
    acl={2021.naacl-main.365},
    pdf={a-dataset-of-information-seeking-questions-and-answers-anchored-in-research-papers.pdf},
    preview={a-dataset-of-information-seeking-questions-and-answers-anchored-in-research-papers.png}
}

@inproceedings{wadden-lo-2021-overview,
    title = "Overview and Insights from the {SCIVER} shared task on Scientific Claim Verification",
    author = "Wadden, David  and
      Lo, Kyle",
    booktitle = "Scholarly Document Processing (SDP) Workshop",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.sdp-1.16",
    pages = "124--129",
    abstract = "We present an overview of the SCIVER shared task, presented at the 2nd Scholarly Document Processing (SDP) workshop at NAACL 2021. In this shared task, systems were provided a scientific claim and a corpus of research abstracts, and asked to identify which articles Support or Refute the claim as well as provide evidentiary sentences justifying those labels. 11 teams made a total of 14 submissions to the shared task leaderboard, leading to an improvement of more than +23 F1 on the primary task evaluation metric. In addition to surveying the participating systems, we provide several insights into modeling approaches to support continued progress and future research on the important and challenging task of scientific claim verification.",
    bibtex_show={true},
    acl={2021.sdp-1.16},
    pdf={overview-and-insights-from-the-sciver-shared-task-on-scientific-claim-verification.pdf},
    preview={overview-and-insights-from-the-sciver-shared-task-on-scientific-claim-verification.png}
}

@inproceedings{beltagy-etal-2021-overview,
    title = "Overview of the Second Workshop on Scholarly Document Processing",
    author = "Beltagy, Iz  and
      Cohan, Arman  and
      Feigenblat, Guy  and
      Freitag, Dayne  and
      Ghosal, Tirthankar  and
      Hall, Keith  and
      Herrmannova, Drahomira  and
      Knoth, Petr  and
      Lo, Kyle  and
      Mayr, Philipp  and
      Patton, Robert  and
      Shmueli-Scheuer, Michal  and
      de Waard, Anita  and
      Wang, Kuansan  and
      Wang, Lucy Lu",
    booktitle = "Scholarly Document Processing (SDP) Workshop",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.sdp-1.22",
    pages = "159--165",
    abstract = "With the ever-increasing pace of research and high volume of scholarly communication, scholars face a daunting task. Not only must they keep up with the growing literature in their own and related fields, scholars increasingly also need to rebut pseudo-science and disinformation. These needs have motivated an increasing focus on computational methods for enhancing search, summarization, and analysis of scholarly documents. However, the various strands of research on scholarly document processing remain fragmented. To reach out to the broader NLP and AI/ML community, pool distributed efforts in this area, and enable shared access to published research, we held the 2nd Workshop on Scholarly Document Processing (SDP) at NAACL 2021 as a virtual event (https://sdproc.org/2021/). The SDP workshop consisted of a research track, three invited talks, and three Shared Tasks (LongSumm 2021, SCIVER, and 3C). The program was geared towards the application of NLP, information retrieval, and data mining for scholarly documents, with an emphasis on identifying and providing solutions to open challenges.",
    bibtex_show={true},
    acl={2021.sdp-1.22},
    pdf={overview-of-the-second-workshop-on-scholarly-document-processing.pdf},
    preview={overview-of-the-second-workshop-on-scholarly-document-processing.png}
}

@inproceedings{10.1145/3411764.3445648,
author = {Head, Andrew and Lo, Kyle and Kang, Dongyeop and Fok, Raymond and Skjonsberg, Sam and Weld, Daniel S. and Hearst, Marti A.},
title = {Augmenting Scientific Papers with Just-in-Time, Position-Sensitive Definitions of Terms and Symbols},
year = {2021},
month = {may},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445648},
doi = {10.1145/3411764.3445648},
abstract = {Despite the central importance of research papers to scientific progress, they can be difficult to read. Comprehension is often stymied when the information needed to understand a passage resides somewhere else—in another section, or in another paper. In this work, we envision how interfaces can bring definitions of technical terms and symbols to readers when and where they need them most. We introduce ScholarPhi, an augmented reading interface with four novel features: (1) tooltips that surface position-sensitive definitions from elsewhere in a paper, (2) a filter over the paper that “declutters” it to reveal how the term or symbol is used across the paper, (3) automatic equation diagrams that expose multiple definitions in parallel, and (4) an automatically generated glossary of important terms and symbols. A usability study showed that the tool helps researchers of all experience levels read papers. Furthermore, researchers were eager to have ScholarPhi’s definitions available to support their everyday reading.},
booktitle = {CHI},
articleno = {413},
numpages = {18},
keywords = {reading interfaces, nonce words, scientific papers, interactive documents, definitions},
location = {Yokohama, Japan},
series = {CHI '21},
selected={true},
bibtex_show={true},
arxiv={2009.14237},
acm={10.1145/3411764.3445648},
pdf={augmenting-scientific-papers-with-just-in-time-position-sensitive-definitions-of-terms-and-symbols.pdf},
preview={augmenting-scientific-papers-with-just-in-time-position-sensitive-definitions-of-terms-and-symbols.png}
}

@inproceedings{gabriel-etal-2021-discourse,
    title = "Discourse Understanding and Factual Consistency in Abstractive Summarization",
    author = "Gabriel, Saadia  and
      Bosselut, Antoine  and
      Da, Jeff  and
      Holtzman, Ari  and
      Buys, Jan  and
      Lo, Kyle  and
      Celikyilmaz, Asli  and
      Choi, Yejin",
    booktitle = "EACL",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.34",
    doi = "10.18653/v1/2021.eacl-main.34",
    pages = "435--447",
    abstract = "We introduce a general framework for abstractive summarization with factual consistency and distinct modeling of the narrative flow in an output summary. Our work addresses current limitations of models for abstractive summarization that often hallucinate information or generate summaries with coherence issues. To generate abstractive summaries with factual consistency and narrative flow, we propose Cooperative Generator-Discriminator Networks (Co-opNet), a novel transformer-based framework where the generator works with a discriminator architecture to compose coherent long-form summaries. We explore four different discriminator objectives which each capture a different aspect of coherence, including whether salient spans of generated abstracts are hallucinated or appear in the input context, and the likelihood of sentence adjacency in generated abstracts. We measure the ability of Co-opNet to learn these objectives with arXiv scientific papers, using the abstracts as a proxy for gold long-form scientific article summaries. Empirical results from automatic and human evaluations demonstrate that Co-opNet learns to summarize with considerably improved global coherence compared to competitive baselines.",
    bibtex_show={true},
    acl={2021.eacl-main.34},
    pdf={discourse-understanding-and-factual-consistency-in-abstractive-summarization.pdf},
    preview={discourse-understanding-and-factual-consistency-in-abstractive-summarization.png}
}

@article{ROBERTS2021103865,
title = {Searching for scientific evidence in a pandemic: An overview of TREC-COVID},
journal = {Journal of Biomedical Informatics},
volume = {121},
pages = {103865},
year = {2021},
month={apr},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103865},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421001945},
author = {Kirk Roberts and Tasmeer Alam and Steven Bedrick and Dina Demner-Fushman and Kyle Lo and Ian Soboroff and Ellen Voorhees and Lucy Lu Wang and William R. Hersh},
keywords = {Information retrieval, COVID-19, Pandemics, TREC-COVID},
abstract = {We present an overview of the TREC-COVID Challenge, an information retrieval (IR) shared task to evaluate search on scientific literature related to COVID-19. The goals of TREC-COVID include the construction of a pandemic search test collection and the evaluation of IR methods for COVID-19. The challenge was conducted over five rounds from April to July 2020, with participation from 92 unique teams and 556 individual submissions. A total of 50 topics (sets of related queries) were used in the evaluation, starting at 30 topics for Round 1 and adding 5 new topics per round to target emerging topics at that state of the still-emerging pandemic. This paper provides a comprehensive overview of the structure and results of TREC-COVID. Specifically, the paper provides details on the background, task structure, topic structure, corpus, participation, pooling, assessment, judgments, results, top-performing systems, lessons learned, and benchmark datasets.},
    bibtex_show={true},
    arxiv={2104.09632},
    sciencedirect={S1532046421001945},
    pdf={searching-for-scientific-evidence-in-a-pandemic-an-overview-of-trec-covid.pdf},
    preview={searching-for-scientific-evidence-in-a-pandemic-an-overview-of-trec-covid.png}
}

@article{10.1145/3451964.3451965,
author = {Voorhees, Ellen and Alam, Tasmeer and Bedrick, Steven and Demner-Fushman, Dina and Hersh, William R. and Lo, Kyle and Roberts, Kirk and Soboroff, Ian and Wang, Lucy Lu},
title = {TREC-COVID: Constructing a Pandemic Information Retrieval Test Collection},
year = {2021},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/3451964.3451965},
doi = {10.1145/3451964.3451965},
abstract = {TREC-COVID is a community evaluation designed to build a test collection that captures the information needs of biomedical researchers using the scientific literature during a pandemic. One of the key characteristics of pandemic search is the accelerated rate of change: the topics of interest evolve as the pandemic progresses and the scientific literature in the area explodes. The COVID-19 pandemic provides an opportunity to capture this progression as it happens. TREC-COVID, in creating a test collection around COVID-19 literature, is building infrastructure to support new research and technologies in pandemic search.},
journal = {SIGIR Forum},
month = {feb},
articleno = {1},
numpages = {12},
  bibtex_show={true},
  arxiv={2005.04474},
  acm={10.1145/3451964.3451965},
  pdf={trec-covid-constructing-a-pandemic-information-retrieval-test-collection.pdf},
  preview={trec-covid-constructing-a-pandemic-information-retrieval-test-collection.png}
}

@article{10.1093/bib/bbaa296,
    author = {Wang, Lucy Lu and Lo, Kyle},
    title = "{Text mining approaches for dealing with the rapidly expanding literature on COVID-19}",
    journal = {Briefings in Bioinformatics},
    volume = {22},
    number = {2},
    pages = {781-799},
    year = {2020},
    month = {dec},
    abstract = "{More than 50 000 papers have been published about COVID-19 since the beginning of 2020 and several hundred new papers continue to be published every day. This incredible rate of scientific productivity leads to information overload, making it difficult for researchers, clinicians and public health officials to keep up with the latest findings. Automated text mining techniques for searching, reading and summarizing papers are helpful for addressing information overload. In this review, we describe the many resources that have been introduced to support text mining applications over the COVID-19 literature; specifically, we discuss the corpora, modeling resources, systems and shared tasks that have been introduced for COVID-19. We compile a list of 39 systems that provide functionality such as search, discovery, visualization and summarization over the COVID-19 literature. For each system, we provide a qualitative description and assessment of the system’s performance, unique data or user interface features and modeling decisions. Many systems focus on search and discovery, though several systems provide novel features, such as the ability to summarize findings over multiple documents or linking between scientific articles and clinical trials. We also describe the public corpora, models and shared tasks that have been introduced to help reduce repeated effort among community members; some of these resources (especially shared tasks) can provide a basis for comparing the performance of different systems. Finally, we summarize promising results and open challenges for text mining the COVID-19 literature.}",
    issn = {1477-4054},
    doi = {10.1093/bib/bbaa296},
    url = {https://doi.org/10.1093/bib/bbaa296},
    eprint = {https://academic.oup.com/bib/article-pdf/22/2/781/36654452/bbaa296.pdf},
    bibtex_show={true},
    openreview={exBoxQwNnY2},
    oup={22/2/781/6024738},
    pdf={text-mining-approaches-for-dealing-with-the-rapidly-expanding-literature-on-covid-19.pdf},
    preview={text-mining-approaches-for-dealing-with-the-rapidly-expanding-literature-on-covid-19.png}
}


@inproceedings{wadden-etal-2020-fact,
    title = "Fact or Fiction: Verifying Scientific Claims",
    author = "Wadden, David  and
      Lin, Shanchuan  and
      Lo, Kyle  and
      Wang, Lucy Lu  and
      van Zuylen, Madeleine  and
      Cohan, Arman  and
      Hajishirzi, Hannaneh",
    booktitle = "EMNLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.609",
    doi = "10.18653/v1/2020.emnlp-main.609",
    pages = "7534--7550",
    abstract = "We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news. We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus. Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at https://github.com/allenai/scifact. A leaderboard and COVID-19 fact-checking demo are available at https://scifact.apps.allenai.org.",
    selected={true},
    bibtex_show={true},
    acl={2020.emnlp-main.609},
    pdf={fact-or-fiction-verifying-scientific-claims.pdf},
    preview={fact-or-fiction-verifying-scientific-claims.png}
}

@inproceedings{kang-etal-2020-document,
    title = "Document-Level Definition Detection in Scholarly Documents: Existing Models, Error Analyses, and Future Directions",
    author = "Kang, Dongyeop  and
      Head, Andrew  and
      Sidhu, Risham  and
      Lo, Kyle  and
      Weld, Daniel  and
      Hearst, Marti A.",
    booktitle = "Scholarly Document Processing (SDP) Workshop",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.sdp-1.22",
    doi = "10.18653/v1/2020.sdp-1.22",
    pages = "196--206",
    abstract = "The task of definition detection is important for scholarly papers, because papers often make use of technical terminology that may be unfamiliar to readers. Despite prior work on definition detection, current approaches are far from being accurate enough to use in realworld applications. In this paper, we first perform in-depth error analysis of the current best performing definition detection system and discover major causes of errors. Based on this analysis, we develop a new definition detection system, HEDDEx, that utilizes syntactic features, transformer encoders, and heuristic filters, and evaluate it on a standard sentence-level benchmark. Because current benchmarks evaluate randomly sampled sentences, we propose an alternative evaluation that assesses every sentence within a document. This allows for evaluating recall in addition to precision. HEDDEx outperforms the leading system on both the sentence-level and the document-level tasks, by 12.7 F1 points and 14.4 F1 points, respectively. We note that performance on the high-recall document-level task is much lower than in the standard evaluation approach, due to the necessity of incorporation of document structure as features. We discuss remaining challenges in document-level definition detection, ideas for improvements, and potential issues for the development of reading aid applications.",
    bibtex_show={true},
    acl={2020.sdp-1.22},
    pdf={document-level-definition-detection-in-scholarly-documents-existing-models-error-analyses-and-future-directions.pdf},
    preview={document-level-definition-detection-in-scholarly-documents-existing-models-error-analyses-and-future-directions.png}
}

@inproceedings{cachola-etal-2020-tldr,
    title = "{TLDR}: Extreme Summarization of Scientific Documents",
    author = "Cachola, Isabel  and
      Lo, Kyle  and
      Cohan, Arman  and
      Weld, Daniel",
    booktitle = "Findings of EMNLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.428",
    doi = "10.18653/v1/2020.findings-emnlp.428",
    pages = "4766--4777",
    abstract = "We introduce TLDR generation, a new form of extreme summarization, for scientific papers. TLDR generation involves high source compression and requires expert background knowledge and understanding of complex domain-specific language. To facilitate study on this task, we introduce SCITLDR, a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SCITLDR contains both author-written and expert-derived TLDRs, where the latter are collected using a novel annotation protocol that produces high-quality summaries while minimizing annotation burden. We propose CATTS, a simple yet effective learning strategy for generating TLDRs that exploits titles as an auxiliary training signal. CATTS improves upon strong baselines under both automated metrics and human evaluations. Data and code are publicly available at https://github.com/allenai/scitldr.",
    selected={true},
    bibtex_show={true},
    acl={2020.findings-emnlp.428},
    pdf={tldr-extreme-summarization-of-scientific-documents.pdf},
    preview={tldr-extreme-summarization-of-scientific-documents.png}
}

@article{10.3389/frma.2020.596624,
author={Kanakia, Anshul and Wang, Kuansan and Dong, Yuxiao and Xie, Boya and Lo, Kyle and Shen, Zhihong and Wang, Lucy Lu and Huang, Chiyuan and Eide, Darrin and Kohlmeier, Sebastian and Wu, Chieh-Han},
title={Mitigating Biases in CORD-19 for Analyzing COVID-19 Literature},
journal={Frontiers in Research Metrics and Analytics},
volume={5},
year={2020},
month={nov},
url={https://www.frontiersin.org/articles/10.3389/frma.2020.596624},
doi={10.3389/frma.2020.596624},
issn={2504-0537},
abstract={On the behest of the Office of Science and Technology Policy in the White House, six institutions, including ours, have created an open research dataset called COVID-19 Research Dataset (CORD-19) to facilitate the development of question-answering systems that can assist researchers in finding relevant research on COVID-19. As of May 27, 2020, CORD-19 includes more than 100,000 open access publications from major publishers and PubMed as well as preprint articles deposited into medRxiv, bioRxiv, and arXiv. Recent years, however, have also seen question-answering and other machine learning systems exhibit harmful behaviors to humans due to biases in the training data. It is imperative and only ethical for modern scientists to be vigilant in inspecting and be prepared to mitigate the potential biases when working with any datasets. This article describes a framework to examine biases in scientific document collections like CORD-19 by comparing their properties with those derived from the citation behaviors of the entire scientific community. In total, three expanded sets are created for the analyses: 1) the enclosure set CORD-19E composed of CORD-19 articles and their references and citations, mirroring the methodology used in the renowned “A Century of Physics” analysis; 2) the full closure graph CORD-19C that recursively includes references starting with CORD-19; and 3) the inflection closure CORD-19I, that is, a much smaller subset of CORD-19C but already appropriate for statistical analysis based on the theory of the scale-free nature of the citation network. Taken together, all these expanded datasets show much smoother trends when used to analyze global COVID-19 research. The results suggest that while CORD-19 exhibits a strong tilt toward recent and topically focused articles, the knowledge being explored to attack the pandemic encompasses a much longer time span and is very interdisciplinary. A question-answering system with such expanded scope of knowledge may perform better in understanding the literature and answering related questions. However, while CORD-19 appears to have topical coverage biases compared to the expanded sets, the collaboration patterns, especially in terms of team sizes and geographical distributions, are captured very well already in CORD-19 as the raw statistics and trends agree with those from larger datasets.},
  bibtex_show={true},
  pmc={PMC8025972},
  pdf={mitigating-biases-in-cord-19-for-analyzing-covid-19-literature.pdf},
  preview={mitigating-biases-in-cord-19-for-analyzing-covid-19-literature.png}
}

@inproceedings{wang-etal-2020-cord,
    title = "{CORD-19}: The {COVID-19} Open Research Dataset",
    author = "Wang, Lucy Lu  and
      Lo, Kyle  and
      Chandrasekhar, Yoganand  and
      Reas, Russell  and
      Yang, Jiangjiang  and
      Burdick, Doug  and
      Eide, Darrin  and
      Funk, Kathryn  and
      Katsis, Yannis  and
      Kinney, Rodney Michael  and
      Li, Yunyao  and
      Liu, Ziyang  and
      Merrill, William  and
      Mooney, Paul  and
      Murdick, Dewey A.  and
      Rishi, Devvret  and
      Sheehan, Jerry  and
      Shen, Zhihong  and
      Stilson, Brandon  and
      Wade, Alex D.  and
      Wang, Kuansan  and
      Wang, Nancy Xin Ru  and
      Wilhelm, Christopher  and
      Xie, Boya  and
      Raymond, Douglas M.  and
      Weld, Daniel S.  and
      Etzioni, Oren  and
      Kohlmeier, Sebastian",
    booktitle = "NLP for COVID-19 Workshop",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.nlpcovid19-acl.1",
    abstract = "The COVID-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 200K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.",
    bibtex_show={true},
    acl={2020.nlpcovid19-acl.1},
    pdf={cord-19-the-covid-19-open-research-dataset.pdf},
    preview={cord-19-the-covid-19-open-research-dataset.png}
}

@inproceedings{lo-etal-2020-s2orc,
    title = "{S}2{ORC}: The Semantic Scholar Open Research Corpus",
    author = "Lo, Kyle  and
      Wang, Lucy Lu  and
      Neumann, Mark  and
      Kinney, Rodney  and
      Weld, Daniel",
    booktitle = "ACL",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.447",
    doi = "10.18653/v1/2020.acl-main.447",
    pages = "4969--4983",
    abstract = "We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.",
    selected={true},
    bibtex_show={true},
    acl={2020.acl-main.447},
    pdf={s2orc-the-semantic-scholar-open-research-corpus.pdf},
    preview={s2orc-the-semantic-scholar-open-research-corpus.png}
}

@inproceedings{gururangan-etal-2020-dont,
    title = "Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks",
    author = "Gururangan, Suchin  and
      Marasovi{\'c}, Ana  and
      Swayamdipta, Swabha  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Downey, Doug  and
      Smith, Noah A.",
    booktitle = "ACL",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.740",
    doi = "10.18653/v1/2020.acl-main.740",
    pages = "8342--8360",
    abstract = "Language models pretrained on text from a wide variety of sources form the foundation of today{'}s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task{'}s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.",
    selected={true},
    bibtex_show={true},
    acl={2020.acl-main.740},
    pdf={dont-stop-pretraining-adapt-language-models-to-domains-and-tasks.pdf},
    preview={dont-stop-pretraining-adapt-language-models-to-domains-and-tasks.png}
}

@article{Lee2020LIMEADEFA,
  title={LIMEADE: From AI Explanations to Advice Taking},
  author={B. Lee and Doug Downey and Kyle Lo and Daniel S. Weld},
  journal={ArXiv},
  year={2020},
  month={mar},
  day={9},
  volume={abs/2003.04315},
  abstract={Research in human-centered AI has shown the benefits of systems that can explain their predictions. Methods that allow an AI to take advice from humans in response to explanations are similarly useful. While both capabilities are well-developed for transparent learning models (e.g., linear models and GA2Ms), and recent techniques (e.g., LIME and SHAP) can generate explanations for opaque models, little attention has been given to advice methods for opaque models. This paper introduces LIMEADE, the first general framework that translates both positive and negative advice (expressed using high-level vocabulary such as that employed by post-hoc explanations) into an update to an arbitrary, underlying opaque model. We demonstrate the generality of our approach with case studies on seventy real-world models across two broad domains: image classification and text recommendation. We show our method improves accuracy compared to a rigorous baseline on the image classification domains. For the text modality, we apply our framework to a neural recommender system for scientific papers on a public website; our user study shows that our framework leads to significantly higher perceived user control, trust, and satisfaction.},
  bibtex_show={true},
  arxiv={2003.04315},
  pdf={limeade-from-ai-explanations-to-advice-taking.pdf},
  preview={limeade-from-ai-explanations-to-advice-taking.png}
}


@inproceedings{beltagy-etal-2019-scibert,
    title = "{S}ci{BERT}: A Pretrained Language Model for Scientific Text",
    author = "Beltagy, Iz  and
      Lo, Kyle  and
      Cohan, Arman",
    booktitle = "EMNLP",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1371",
    doi = "10.18653/v1/D19-1371",
    pages = "3615--3620",
    abstract = "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.",
    selected={true},
    bibtex_show={true},
    acl={D19-1371},
    pdf={scibert-a-pretrained-language-model-for-scientific-text.pdf},
    preview={scibert-a-pretrained-language-model-for-scientific-text.png}
}


@article{10.1001/jamanetworkopen.2019.6700,
    author = {Feldman, Sergey and Ammar, Waleed and Lo, Kyle and Trepman, Elly and van Zuylen, Madeleine and Etzioni, Oren},
    title = "{Quantifying Sex Bias in Clinical Studies at Scale With Automated Data Extraction}",
    journal = {JAMA Network Open},
    volume = {2},
    number = {7},
    pages = {e196700-e196700},
    year = {2019},
    month = {jul},
    abstract = "{Analyses of female representation in clinical studies have been limited in scope and scale.To perform a large-scale analysis of global enrollment sex bias in clinical studies.In this cross-sectional study, clinical studies from published articles from PubMed from 1966 to 2018 and records from Aggregate Analysis of ClinicalTrials.gov from 1999 to 2018 were identified. Global disease prevalence was determined for male and female patients in 11 disease categories from the Global Burden of Disease database: cardiovascular, diabetes, digestive, hepatitis (types A, B, C, and E), HIV/AIDS, kidney (chronic), mental, musculoskeletal, neoplasms, neurological, and respiratory (chronic). Machine reading algorithms were developed that extracted sex data from tables in articles and records on December 31, 2018, at an artificial intelligence research institute. Male and female participants in 43 135 articles (792 004 915 participants) and 13 165 records (12 977 103 participants) were included.Sex bias was defined as the difference between the fraction of female participants in study participants minus prevalence fraction of female participants for each disease category. A total of 1000 bootstrap estimates of sex bias were computed by resampling individual studies with replacement. Sex bias was reported as mean and 95\\% bootstrap confidence intervals from articles and records in each disease category over time (before or during 1993 to 2018), with studies or participants as the measurement unit.There were 792 004 915 participants, including 390 470 834 female participants (49\\%), in articles and 12 977 103 participants, including 6 351 619 female participants (49\\%), in records. With studies as measurement unit, substantial female underrepresentation (sex bias ≤ −0.05) was observed in 7 of 11 disease categories, especially HIV/AIDS (mean for articles, −0.17 [95\\% CI, −0.18 to −0.16]), chronic kidney diseases (mean, −0.17 [95\\% CI, −0.17 to −0.16]), and cardiovascular diseases (mean, −0.14 [95\\% CI, −0.14 to −0.13]). Sex bias in articles for all categories combined was unchanged over time with studies as measurement unit (range, −0.15 [95\\% CI, −0.16 to −0.13] to −0.10 [95\\% CI, −0.14 to −0.06]), but improved from before or during 1993 (mean, −0.11 [95\\% CI, −0.16 to −0.05]) to 2014 to 2018 (mean, −0.05 [95\\% CI, −0.09 to −0.02]) with participants as the measurement unit. Larger study size was associated with greater female representation.Automated extraction of the number of participants in clinical reports provides an effective alternative to manual analysis of demographic bias. Despite legal and policy initiatives to increase female representation, sex bias against female participants in clinical studies persists. Studies with more participants have greater female representation. Differences between sex bias estimates with studies vs participants as measurement unit, and between articles vs records, suggest that sex bias with both measures and data sources should be reported.}",
    issn = {2574-3805},
    doi = {10.1001/jamanetworkopen.2019.6700},
    url = {https://doi.org/10.1001/jamanetworkopen.2019.6700},
    eprint = {https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2737103/feldman\_2019\_oi\_190268.pdf},
    bibtex_show={true},
    pmc={PMC6613296},
    jama={2737103},
    pdf={quantifying-sex-bias-in-clinical-studies-at-scale-with-automated-data-extraction.pdf},
    preview={quantifying-sex-bias-in-clinical-studies-at-scale-with-automated-data-extraction.png}
}

@inproceedings{beltagy-etal-2019-combining,
    title = "Combining Distant and Direct Supervision for Neural Relation Extraction",
    author = "Beltagy, Iz  and
      Lo, Kyle  and
      Ammar, Waleed",
    booktitle = "NAACL",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1184",
    doi = "10.18653/v1/N19-1184",
    pages = "1858--1867",
    abstract = "In relation extraction with distant supervision, noisy labels make it difficult to train quality models. Previous neural models addressed this problem using an attention mechanism that attends to sentences that are likely to express the relations. We improve such models by combining the distant supervision data with an additional directly-supervised data, which we use as supervision for the attention weights. We find that joint training on both types of supervision leads to a better model because it improves the model{'}s ability to identify noisy sentences. In addition, we find that sigmoidal attention weights with max pooling achieves better performance over the commonly used weighted average attention in this setup. Our proposed method achieves a new state-of-the-art result on the widely used FB-NYT dataset.",
    bibtex_show={true},
    acl={N19-1184},
    pdf={combining-distant-and-direct-supervision-for-neural-relation-extraction.pdf},
    preview={combining-distant-and-direct-supervision-for-neural-relation-extraction.png}
}

@inproceedings{wang-etal-2018-ontology,
    title = "Ontology alignment in the biomedical domain using entity definitions and context",
    author = "Wang, Lucy Lu  and
      Bhagavatula, Chandra  and
      Neumann, Mark  and
      Lo, Kyle  and
      Wilhelm, Chris  and
      Ammar, Waleed",
    booktitle = "BioNLP Workshop",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-2306",
    doi = "10.18653/v1/W18-2306",
    pages = "47--55",
    abstract = "Ontology alignment is the task of identifying semantically equivalent entities from two given ontologies. Different ontologies have different representations of the same entity, resulting in a need to de-duplicate entities when merging ontologies. We propose a method for enriching entities in an ontology with external definition and context information, and use this additional information for ontology alignment. We develop a neural architecture capable of encoding the additional information when available, and show that the addition of external data results in an F1-score of 0.69 on the Ontology Alignment Evaluation Initiative (OAEI) largebio SNOMED-NCI subtask, comparable with the entity-level matchers in a SOTA system.",
    bibtex_show={true},
    acl={W18-2306},
    pdf={ontology-alignment-in-the-biomedical-domain-using-entity-definitions-and-context.pdf},
    preview={ontology-alignment-in-the-biomedical-domain-using-entity-definitions-and-context.png}
}

@inproceedings{ammar-etal-2018-construction,
    title = "Construction of the Literature Graph in Semantic Scholar",
    author = "Ammar, Waleed  and
      Groeneveld, Dirk  and
      Bhagavatula, Chandra  and
      Beltagy, Iz  and
      Crawford, Miles  and
      Downey, Doug  and
      Dunkelberger, Jason  and
      Elgohary, Ahmed  and
      Feldman, Sergey  and
      Ha, Vu  and
      Kinney, Rodney  and
      Kohlmeier, Sebastian  and
      Lo, Kyle  and
      Murray, Tyler  and
      Ooi, Hsu-Han  and
      Peters, Matthew  and
      Power, Joanna  and
      Skjonsberg, Sam  and
      Wang, Lucy Lu  and
      Wilhelm, Chris  and
      Yuan, Zheng  and
      van Zuylen, Madeleine  and
      Etzioni, Oren",
    booktitle = "NAACL",
    month = jun,
    year = "2018",
    address = "New Orleans - Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-3011",
    doi = "10.18653/v1/N18-3011",
    pages = "84--91",
    abstract = "We describe a deployed scalable system for organizing published scientific literature into a heterogeneous graph to facilitate algorithmic manipulation and discovery. The resulting literature graph consists of more than 280M nodes, representing papers, authors, entities and various interactions between them (e.g., authorships, citations, entity mentions). We reduce literature graph construction into familiar NLP tasks (e.g., entity extraction and linking), point out research challenges due to differences from standard formulations of these tasks, and report empirical results for each task. The methods described in this paper are used to enable semantic features in \url{www.semanticscholar.org}.",
    bibtex_show={true},
    acl={N18-3011},
    pdf={construction-of-the-literature-graph-in-semantic-scholar.pdf},
    preview={construction-of-the-literature-graph-in-semantic-scholar.png}
}

