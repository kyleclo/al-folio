---
layout: page
title: Adapting Language Models to Specialized Texts
# description: How do we adapt language models trained on broad web crawls to perform well on specialized texts, like scientific articles?
# img: assets/img/3.jpg
importance: 2
category: work
---

<!-- How do we adapt language models trained on broad web crawls to perform well on specialized texts, like scientific articles? -->


I was a developer of [SciBERT](https://aclanthology.org/D19-1371), one of the first pretrained language models for scientific text.
Our follow-on work on domain adaptation via [continued pretraining](https://aclanthology.org/2020.acl-main.740/) won an honorable mention for best paper at ACL 2020 üèÜ.

I also develop methods for [infusing language models with visual layout](https://aclanthology.org/2022.tacl-1.22/).
I packaged these models into [PaperMage](https://aclanthology.org/2023.emnlp-demo.45/), an open-source Python library that won best paper at ACL 2023 System Demos üèÜ.

Since 2023, I've been working on out-of-domain generalization via [parameter-efficient training](https://arxiv.org/abs/2311.09765) and [data augmentation](https://arxiv.org/abs/2309.08541).